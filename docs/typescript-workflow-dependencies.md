# TypeScript Workflow Dependency Management

## Problem Statement

When workflows are deployed on VMs with:
- Agent running in: `C:/abcd/agent/`
- Workflows mounted from S3: `S:/orgaabcd/workflowabcd/`

We need to:
1. Install dependencies efficiently
2. Cache node_modules to reduce delays
3. Validate workflow structure
4. Handle version mismatches
5. Persist cache even when VM restarts

## Proposed Solution

### 1. Workflow Structure Validation

**Enforce strict structure:**
```
S:/orgaabcd/workflowabcd/
├── terminator.ts          # Required: Workflow entrypoint
├── package.json           # Required: Dependencies manifest
├── bun.lockb             # Auto-generated by bun
├── node_modules/         # Auto-generated (see caching strategy)
├── scripts/              # Optional: Helper scripts
└── utils/                # Optional: Shared utilities
```

**Rules:**
- ✅ **MUST** have `terminator.ts` as entrypoint (easy to find, consistent)
- ✅ **MUST** have `package.json` with dependencies
- ✅ **MUST** have only ONE workflow per folder (throw error otherwise)
- ✅ Auto-generate `bun.lockb` on first install
- ❌ **ERROR** if multiple `*.workflow.ts` or conflicting exports

### 2. Dependency Cache Strategy

**Three-tier caching:**

#### Option A: Local VM Cache (Fast, but VM-specific)
```
C:/abcd/agent/
└── .workflow-cache/
    └── <workflow-hash>/
        ├── node_modules/
        ├── bun.lockb
        └── .cache-metadata.json
```

**Pros:**
- Fast access (local disk)
- Isolated per workflow (hash-based)
- No S3 write costs

**Cons:**
- Lost when VM restarts
- Not shared across VMs
- Cold start every VM launch

#### Option B: S3-Persisted Cache (Slow, but persistent)
```
S:/orgaabcd/workflowabcd/
├── terminator.ts
├── package.json
├── bun.lockb              # Persisted to S3
└── node_modules/          # Persisted to S3
    └── ... (all deps)
```

**Pros:**
- Survives VM restarts
- Shared cache across VMs running same workflow
- Simple implementation

**Cons:**
- Slow S3 I/O for node_modules (thousands of small files)
- Large S3 storage costs
- Network latency on every file access

#### Option C: **Hybrid Strategy (RECOMMENDED)**
```
# S3 (persistent, slow)
S:/orgaabcd/workflowabcd/
├── terminator.ts
├── package.json
├── bun.lockb              # Persisted lock file
└── .deps-hash             # Hash of package.json + bun.lockb

# Local VM (fast, ephemeral)
C:/abcd/agent/
└── .workflow-cache/
    └── <deps-hash>/
        └── node_modules/   # Copied from S3 or installed fresh
```

**How it works:**
1. Check if `.deps-hash` exists in S3 workflow folder
2. If exists, check if local cache `C:/abcd/agent/.workflow-cache/<deps-hash>/` exists
3. If local cache exists → use it (instant)
4. If local cache missing → install fresh with `bun install`
5. Save `bun.lockb` back to S3 (for consistency)
6. Update `.deps-hash` in S3

**Pros:**
- Fast: Local node_modules access
- Persistent: Hash stored in S3, survives VM restart
- Shared: Multiple VMs with same workflow share hash
- Efficient: Only install when package.json changes

**Cons:**
- Slightly more complex logic
- Still cold start on new VM (but only once per workflow version)

### 3. Dependency Installation Flow

```rust
// workflow_typescript.rs

async fn ensure_dependencies(workflow_path: &Path) -> Result<()> {
    // 1. Validate workflow structure
    let entrypoint = workflow_path.join("terminator.ts");
    if !entrypoint.exists() {
        return Err(anyhow!("Missing required entrypoint: terminator.ts"));
    }

    let package_json = workflow_path.join("package.json");
    if !package_json.exists() {
        return Err(anyhow!("Missing required package.json"));
    }

    // 2. Calculate dependency hash
    let deps_hash = calculate_deps_hash(&package_json).await?;

    // 3. Check local cache
    let cache_dir = get_cache_dir(&deps_hash)?;
    if cache_dir.join("node_modules").exists() {
        info!("Using cached dependencies: {}", deps_hash);
        return Ok(());
    }

    // 4. Install dependencies
    info!("Installing dependencies for workflow...");
    let install_result = Command::new("bun")
        .arg("install")
        .arg("--frozen-lockfile")  // Use existing lockfile if present
        .current_dir(workflow_path)
        .output()
        .await?;

    if !install_result.status.success() {
        return Err(anyhow!("Dependency installation failed: {}",
            String::from_utf8_lossy(&install_result.stderr)));
    }

    // 5. Copy node_modules to local cache
    copy_dir_recursive(
        &workflow_path.join("node_modules"),
        &cache_dir.join("node_modules")
    ).await?;

    // 6. Save lockfile and hash to S3 (for persistence)
    save_deps_hash_to_s3(workflow_path, &deps_hash).await?;

    info!("Dependencies installed and cached: {}", deps_hash);
    Ok(())
}

fn calculate_deps_hash(package_json: &Path) -> Result<String> {
    let package_content = fs::read_to_string(package_json)?;

    // Include bun.lockb if exists (for consistency)
    let lockfile = package_json.parent()
        .unwrap()
        .join("bun.lockb");

    let lockfile_content = if lockfile.exists() {
        fs::read(&lockfile)?
    } else {
        vec![]
    };

    // Hash both files
    let mut hasher = Sha256::new();
    hasher.update(package_content.as_bytes());
    hasher.update(&lockfile_content);

    Ok(format!("{:x}", hasher.finalize()))
}

fn get_cache_dir(deps_hash: &str) -> Result<PathBuf> {
    let agent_dir = env::var("TERMINATOR_AGENT_DIR")
        .unwrap_or_else(|_| "C:/terminator-agent".to_string());

    let cache_dir = PathBuf::from(agent_dir)
        .join(".workflow-cache")
        .join(deps_hash);

    fs::create_dir_all(&cache_dir)?;
    Ok(cache_dir)
}
```

### 4. Workflow Execution with Cache

```rust
async fn execute_typescript_workflow(workflow_path: &Path, inputs: &Value) -> Result<Value> {
    // 1. Ensure dependencies are installed
    ensure_dependencies(workflow_path).await?;

    // 2. Find entrypoint (always terminator.ts)
    let entrypoint = workflow_path.join("terminator.ts");

    // 3. Execute with bun
    let result = Command::new("bun")
        .arg("run")
        .arg(&entrypoint)
        .arg("--inputs")
        .arg(serde_json::to_string(inputs)?)
        .current_dir(workflow_path)
        .env("NODE_ENV", "production")
        .output()
        .await?;

    // 4. Parse output
    parse_workflow_output(&result.stdout)
}
```

### 5. Package Version Validation

**Add to package.json:**
```json
{
  "name": "my-workflow",
  "version": "1.0.0",
  "dependencies": {
    "@mediar/terminator-workflow": "^0.1.0",
    "terminator.js": "^0.1.0"
  },
  "engines": {
    "bun": ">=1.0.0"
  }
}
```

**Validation in Rust:**
```rust
fn validate_package_json(package_json: &Path) -> Result<()> {
    let content = fs::read_to_string(package_json)?;
    let pkg: Value = serde_json::from_str(&content)?;

    // Check required dependencies
    let deps = pkg.get("dependencies")
        .ok_or_else(|| anyhow!("Missing dependencies in package.json"))?;

    if !deps.get("@mediar/terminator-workflow").is_some() {
        return Err(anyhow!(
            "Missing required dependency: @mediar/terminator-workflow"
        ));
    }

    // Check for conflicting workflows
    let workflow_files: Vec<_> = glob::glob(
        package_json.parent().unwrap().join("*.workflow.ts").to_str().unwrap()
    )?
    .collect();

    if workflow_files.len() > 1 {
        return Err(anyhow!(
            "Multiple workflow files found. Only one workflow per folder allowed."
        ));
    }

    Ok(())
}
```

### 6. Cache Cleanup Strategy

**Background cleanup:**
```rust
// Periodically clean old cache entries
async fn cleanup_old_cache() {
    let cache_dir = PathBuf::from("C:/abcd/agent/.workflow-cache");

    for entry in fs::read_dir(&cache_dir)? {
        let entry = entry?;
        let metadata = entry.metadata()?;

        // Remove caches older than 7 days
        if let Ok(modified) = metadata.modified() {
            if modified.elapsed()? > Duration::from_secs(7 * 24 * 60 * 60) {
                fs::remove_dir_all(entry.path())?;
                info!("Cleaned old cache: {:?}", entry.path());
            }
        }
    }
}
```

## Implementation Plan

### Phase 1: Validation ✅ COMPLETE
- [x] Enforce `terminator.ts` entrypoint
- [x] Validate single workflow per folder
- [x] Validate package.json structure (checks existence)

### Phase 2: Local Cache ✅ COMPLETE
- [x] Implement dependency hash calculation (SHA256)
- [x] Create local cache directory structure
- [x] Copy node_modules to cache after install
- [x] Check cache before installing
- [x] Support configurable cache directory (TERMINATOR_CACHE_DIR)

### Phase 3: S3 Persistence (Future Enhancement)
- [ ] Save `.deps-hash` to S3
- [ ] Save `bun.lockb` to S3
- [ ] Check S3 hash before local cache

### Phase 4: Optimization (Future Enhancement)
- [ ] Parallel dependency installation
- [ ] Cache cleanup background task
- [ ] Metrics (cache hit rate, install time)

## Testing Status

- ✅ 11/11 unit tests passing
- ✅ Validation tests (terminator.ts requirement, single workflow)
- ✅ Hash calculation tests (deterministic, with/without lockfile)
- ✅ Cache directory tests
- ✅ Recursive copy tests
- ✅ Integration test structure created

See `docs/typescript-workflow-testing.md` for full test results.

## Trade-offs Summary

| Strategy | Speed | Persistence | Complexity | Cost |
|----------|-------|-------------|------------|------|
| Local only | ⚡⚡⚡ Fast | ❌ Lost on restart | ✅ Simple | $ Low |
| S3 only | 🐌 Slow | ✅ Persistent | ✅ Simple | $$$ High |
| **Hybrid** | ⚡⚡ Fast | ✅ Persistent | ⚠️ Medium | $$ Medium |

**Recommendation: Hybrid Strategy** - Best balance of speed, persistence, and cost.

## Open Questions

1. **Should we pre-warm cache on VM startup?**
   - Pro: Faster first workflow run
   - Con: Slower VM boot time

2. **Should we share cache across multiple orgs on same VM?**
   - Pro: Save disk space
   - Con: Security risk (different orgs)

3. **Should we version the cache format?**
   - Pro: Easy migration when cache structure changes
   - Con: More complexity

4. **Should we compress node_modules in cache?**
   - Pro: Save disk space
   - Con: Slower access time
